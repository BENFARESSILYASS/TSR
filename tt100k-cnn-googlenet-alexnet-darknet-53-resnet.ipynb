{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":191501,"sourceType":"datasetVersion","datasetId":82373}],"dockerImageVersionId":30140,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install tensorflow opencv-python","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-31T18:10:16.759543Z","iopub.execute_input":"2021-10-31T18:10:16.759916Z","iopub.status.idle":"2021-10-31T18:10:16.782122Z","shell.execute_reply.started":"2021-10-31T18:10:16.75982Z","shell.execute_reply":"2021-10-31T18:10:16.781456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport glob\nfrom sklearn.model_selection import train_test_split\nimport shutil\nimport zipfile\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import InceptionV3, VGG16, ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Divide the folders","metadata":{}},{"cell_type":"code","source":"def split(data_path,train,validation,split_size=0.1):\n    folders = os.listdir(data_path)\n    for f in folders:\n        fullpath = os.path.join(data_path,f)\n        image = glob.glob(os.path.join(fullpath,'*.png'))\n       \n        x_train,x_val = train_test_split(image,test_size = split_size)\n        #print(\"folder name\",f,len(x_train))\n       \n        for x in x_train:\n         #   print(x)\n            path_to_folder = os.path.join(train,f)\n           \n            if not os.path.isdir(path_to_folder):\n                os.makedirs(path_to_folder)\n            shutil.copy(x,path_to_folder)\n       \n        for y in x_val:\n            path_to_folder = os.path.join(validation,f)\n            if not os.path.isdir(path_to_folder):\n                os.makedirs(path_to_folder)\n            shutil.copy(y,path_to_folder)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:10:17.528978Z","iopub.execute_input":"2021-10-31T18:10:17.529251Z","iopub.status.idle":"2021-10-31T18:10:17.536333Z","shell.execute_reply.started":"2021-10-31T18:10:17.529222Z","shell.execute_reply":"2021-10-31T18:10:17.535544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/tt100k-traffic-sign/Train'\nif not os.path.isdir('/kaggle/working/Training'):\n    os.mkdir('/kaggle/working/Training')\nif not os.path.isdir('/kaggle/working/Validation'):\n    os.mkdir('/kaggle/working/Validation')\n\ntrain = '/kaggle/working/Training'\nvalidation = '/kaggle/working/Validation'\nsplit(data_path,train=train,validation=validation,split_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:10:17.53753Z","iopub.execute_input":"2021-10-31T18:10:17.537893Z","iopub.status.idle":"2021-10-31T18:13:26.458782Z","shell.execute_reply.started":"2021-10-31T18:10:17.537861Z","shell.execute_reply":"2021-10-31T18:13:26.457949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Data Test ","metadata":{}},{"cell_type":"code","source":"df =pd.read_csv(\"../input/tt100k-traffic-sign/Test.csv\")\ndf['Path'] = df['Path'].str.replace('Test/','')\ndf.to_csv('/kaggle/working/Test1.csv')\nos.mkdir(\"/kaggle/working/Test\")","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:13:26.460311Z","iopub.execute_input":"2021-10-31T18:13:26.460578Z","iopub.status.idle":"2021-10-31T18:13:26.557515Z","shell.execute_reply.started":"2021-10-31T18:13:26.460544Z","shell.execute_reply":"2021-10-31T18:13:26.556818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_test(path_to_image,path_file):\n\n    with open(path_file,\"r\") as csvfile:\n        r= csv.reader(csvfile,delimiter =',')\n  \n        for i,row in enumerate(r):\n            if i == 0: \n                continue\n            label = row[-2]\n            img_name = row[-1]\n            \n            dest = os.path.join('/kaggle/working/Test/',label)\n            if not os.path.isdir(dest):\n                os.makedirs(dest)\n            \n            to_move = os.path.join(path_to_image,img_name)\n            shutil.copy(to_move,dest)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:13:26.559658Z","iopub.execute_input":"2021-10-31T18:13:26.559948Z","iopub.status.idle":"2021-10-31T18:13:26.567445Z","shell.execute_reply.started":"2021-10-31T18:13:26.559911Z","shell.execute_reply":"2021-10-31T18:13:26.565651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\npath_to_image = \"../input/tt100k-traffic-sign/Test\"\npath_file ='/kaggle/working/Test1.csv'\nprepare_test(path_to_image,path_file)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:13:26.569009Z","iopub.execute_input":"2021-10-31T18:13:26.56976Z","iopub.status.idle":"2021-10-31T18:14:11.831271Z","shell.execute_reply.started":"2021-10-31T18:13:26.569721Z","shell.execute_reply":"2021-10-31T18:14:11.830525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Begin create Model","metadata":{}},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow.keras.layers import Conv2D, Input, Dense, MaxPool2D, Flatten, BatchNormalization, GlobalAvgPool2D,Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom tensorflow.keras import Model\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:14:11.832622Z","iopub.execute_input":"2021-10-31T18:14:11.832901Z","iopub.status.idle":"2021-10-31T18:14:16.402053Z","shell.execute_reply.started":"2021-10-31T18:14:11.832863Z","shell.execute_reply":"2021-10-31T18:14:16.401209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create CNN\n\nI have created 3 Convolutional layers with Maxpool layers in between. For all layers I have maintained same padding to ensure the layer's outputs to have same spatial dimensions as its inputs. This will ensure we dont miss out the info in the edges. These are all small images and we cant afford to miss out the edges\n\nI also tried other activation functions. But only relu provides the highest accuracy\n","metadata":{}},{"cell_type":"code","source":"def run_cnn(number_classes):\n    \n    i_shape = Input(shape=(60,60,3)) #Check the picture sizes randomly and decided. Can also done systematically by checking for all files and using median value\n    x = Conv2D(32, (3,3), activation='relu', padding=\"same\") (i_shape)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(64, (3,3),  activation='relu', padding=\"same\") (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(128, (3,3),  activation='relu', padding=\"same\") (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)  \n    \n    x = Conv2D(128, (3,3),  activation='relu') (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x) \n    \n      \n   # x = Flatten()(x)\n    x = GlobalAvgPool2D()(x)\n    x = Dense(1024,  activation='relu') (x)\n    x = Dense(number_classes,activation=\"softmax\")(x)\n    \n\n    return Model(inputs=i_shape,outputs=x)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:14:16.403266Z","iopub.execute_input":"2021-10-31T18:14:16.403536Z","iopub.status.idle":"2021-10-31T18:14:16.421733Z","shell.execute_reply.started":"2021-10-31T18:14:16.403502Z","shell.execute_reply":"2021-10-31T18:14:16.41997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With Elu activation function","metadata":{}},{"cell_type":"code","source":"def run_cnn_elu(number_classes):\n    \n    i_shape = Input(shape=(60,60,3)) #Check the picture sizes randomly and decided. Can also done systematically by checking for all files and using median value\n    x = Conv2D(32, (3,3), activation='elu', padding=\"same\") (i_shape)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(64, (3,3),  activation='elu', padding=\"same\") (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(128, (3,3),  activation='elu', padding=\"same\") (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x)  \n    \n    x = Conv2D(128, (3,3),  activation='elu') (x)\n    x = MaxPool2D()(x)\n    x = BatchNormalization()(x) \n    \n      \n   # x = Flatten()(x)\n    x = GlobalAvgPool2D()(x)\n    x = Dense(1024,  activation='elu') (x)\n    x = Dense(number_classes,activation=\"softmax\")(x)\n    \n\n    return Model(inputs=i_shape,outputs=x)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:14:16.426437Z","iopub.execute_input":"2021-10-31T18:14:16.426699Z","iopub.status.idle":"2021-10-31T18:14:16.44098Z","shell.execute_reply.started":"2021-10-31T18:14:16.42666Z","shell.execute_reply":"2021-10-31T18:14:16.440328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data augmentation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmentation functions\n\n# Horizontal Flip\ndef horizontal_flip(image):\n    return cv2.flip(image, 1)\n\n# Contrast Adjustment\ndef adjust_contrast(image, alpha=1.5):\n    return cv2.convertScaleAbs(image, alpha=alpha, beta=0)\n\n# Add Gaussian Noise\ndef add_noise(image, mean=0, std=25):\n    noise = np.random.normal(mean, std, image.shape).astype(np.uint8)\n    noisy_image = cv2.add(image, noise)\n    return noisy_image\n\n# Blurring (Gaussian Blur)\ndef blur_image(image, ksize=5):\n    return cv2.GaussianBlur(image, (ksize, ksize), 0)\n\n# Zoom\ndef zoom_image(image, zoom_factor=1.2):\n    h, w = image.shape[:2]\n    new_h, new_w = int(h / zoom_factor), int(w / zoom_factor)\n    resized_image = cv2.resize(image, (new_w, new_h))\n    pad_h = (h - new_h) // 2\n    pad_w = (w - new_w) // 2\n    padded_image = cv2.copyMakeBorder(resized_image, pad_h, pad_h, pad_w, pad_w, cv2.BORDER_REFLECT)\n    return cv2.resize(padded_image, (w, h))\n\n# Function to save augmented image\ndef save_image(image, output_dir, image_name, aug_type):\n    output_path = os.path.join(output_dir, f\"{image_name}_{aug_type}.jpg\")\n    cv2.imwrite(output_path, image)\n\n# Directory paths\ninput_dir = '/input/tt100k-traffic-sign/'  # Set the path to your dataset\noutput_dir = '/kaggle/working/'    # Set the path to save augmented images\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Loop through the dataset\nfor image_name in os.listdir(input_dir):\n    image_path = os.path.join(input_dir, image_name)\n\n    if image_name.endswith('.jpg') or image_name.endswith('.png'):\n        # Read the image\n        image = cv2.imread(image_path)\n\n        # Apply Augmentations\n        flipped_image = horizontal_flip(image)\n        contrast_image = adjust_contrast(image)\n        noisy_image = add_noise(image)\n        blurred_image = blur_image(image)\n        zoomed_image = zoom_image(image)\n\n        # Save the augmented images\n        save_image(flipped_image, output_dir, image_name.split('.')[0], 'flip')\n        save_image(contrast_image, output_dir, image_name.split('.')[0], 'contrast')\n        save_image(noisy_image, output_dir, image_name.split('.')[0], 'noise')\n        save_image(blurred_image, output_dir, image_name.split('.')[0], 'blur')\n        save_image(zoomed_image, output_dir, image_name.split('.')[0], 'zoom')\n\nprint(\"Data augmentation completed and saved to\", output_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train, Validation & Test","metadata":{}},{"cell_type":"code","source":"def create_generators(train_dir,batch_size,validation_dir,test_dir):\n  #  train_datagen1 = ImageDataGenerator(\n   #   rescale=1./255,\n    #  rotation_range=40,\n     # width_shift_range=0.2,\n      #height_shift_range=0.2,\n      #shear_range=0.2,\n      #zoom_range=0.2,\n      #horizontal_flip=True,\n      #fill_mode='nearest')\n     \n    train_datagen = ImageDataGenerator(\n      rescale=1./255)\n    \n    validation_datagen = ImageDataGenerator(rescale=1./255)\n    test_datagen = ImageDataGenerator(rescale=1./255)\n     \n    train_generator=train_datagen.flow_from_directory(\n         train_dir,target_size=(60,60),batch_size = batch_size,\n         color_mode = 'rgb',class_mode =\"categorical\",\n         shuffle=True)\n     \n    validation_generator=validation_datagen.flow_from_directory(\n         validation_dir,target_size=(60,60),batch_size = batch_size,\n         color_mode = 'rgb',class_mode =\"categorical\",\n         shuffle=True)\n     \n    test_generator=test_datagen.flow_from_directory(\n         test_dir,target_size=(60,60),batch_size = batch_size,\n         color_mode = 'rgb',class_mode =\"categorical\",\n         shuffle=True) \n     \n    return(train_generator,validation_generator,test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:14:16.442297Z","iopub.execute_input":"2021-10-31T18:14:16.442579Z","iopub.status.idle":"2021-10-31T18:14:16.453224Z","shell.execute_reply.started":"2021-10-31T18:14:16.442543Z","shell.execute_reply":"2021-10-31T18:14:16.452558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fit the model","metadata":{}},{"cell_type":"code","source":"train_dir = train\nvalidation_dir = validation\ntest_dir = '/kaggle/working/Test'\nbatch_size = 60\nnumber_classes = 43 \n\nos.mkdir('/kaggle/working/model1')\npath_to_save_model = '/kaggle/working/model1'\nchkpt_saver = ModelCheckpoint(\n    path_to_save_model, monitor='val_accuracy',\n    mode ='max',save_best_only= True,\n    save_freq='epoch',verbose = 1\n    )\n\n\n \ntrain_generator,validation_generator,test_generator = create_generators(train_dir,batch_size,validation_dir,test_dir)  \nmodel =   run_cnn(number_classes)\nmodel.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\nhistory = model.fit(train_generator, # 2000 images = batch_size * steps\n      epochs=15,batch_size = batch_size,\n      validation_data=validation_generator, callbacks=[chkpt_saver]\n    ) \n  \n     \nmodel = tensorflow.keras.models.load_model(path_to_save_model)  \n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:14:16.454615Z","iopub.execute_input":"2021-10-31T18:14:16.455738Z","iopub.status.idle":"2021-10-31T18:21:33.905947Z","shell.execute_reply.started":"2021-10-31T18:14:16.455701Z","shell.execute_reply":"2021-10-31T18:21:33.904828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = train\nvalidation_dir = validation\ntest_dir = '/kaggle/working/Test'\nbatch_size = 60\nnumber_classes = 43 \n\nos.mkdir('/kaggle/working/model_n')\npath_to_save_model = '/kaggle/working/model_n'\nchkpt_saver = ModelCheckpoint(\n    path_to_save_model, monitor='val_accuracy',\n    mode ='max',save_best_only= True,\n    save_freq='epoch',verbose = 1\n    )\n\n\n \ntrain_generator,validation_generator,test_generator = create_generators(train_dir,batch_size,validation_dir,test_dir)  \nmodel_n =   run_cnn_elu(number_classes)\nmodel_n.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\nhistory = model_n.fit(train_generator, # 2000 images = batch_size * steps\n      epochs=15,batch_size = batch_size,\n      validation_data=validation_generator, callbacks=[chkpt_saver]\n    ) \n  \n     \nmodel_n = tensorflow.keras.models.load_model(path_to_save_model)  ","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:31:26.18163Z","iopub.execute_input":"2021-10-31T18:31:26.18196Z","iopub.status.idle":"2021-10-31T18:39:02.537678Z","shell.execute_reply.started":"2021-10-31T18:31:26.181919Z","shell.execute_reply":"2021-10-31T18:39:02.531404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"validation set\")\nmodel.evaluate(validation_generator)\nprint(\"test set\")\nmodel.evaluate(test_generator)\n\n \n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:30:00.606035Z","iopub.execute_input":"2021-10-31T18:30:00.606231Z","iopub.status.idle":"2021-10-31T18:30:13.203279Z","shell.execute_reply.started":"2021-10-31T18:30:00.606207Z","shell.execute_reply":"2021-10-31T18:30:13.202481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"validation set\")\nmodel_n.evaluate(validation_generator)\nprint(\"test set\")\nmodel_n.evaluate(test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:39:49.846959Z","iopub.execute_input":"2021-10-31T18:39:49.847707Z","iopub.status.idle":"2021-10-31T18:39:58.635349Z","shell.execute_reply.started":"2021-10-31T18:39:49.847661Z","shell.execute_reply":"2021-10-31T18:39:58.634647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_1_fnames = os.listdir( '/kaggle/working/Training/1' )\ntrain_2_fnames = os.listdir( '/kaggle/working/Training/2' )\n\nprint(train_1_fnames[:10])\nprint(train_2_fnames[:10])\n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:30:13.2114Z","iopub.execute_input":"2021-10-31T18:30:13.213162Z","iopub.status.idle":"2021-10-31T18:30:13.226453Z","shell.execute_reply.started":"2021-10-31T18:30:13.213124Z","shell.execute_reply":"2021-10-31T18:30:13.225642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n\nvisualization_model = tensorflow.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n\nsign1_files = [os.path.join('/kaggle/working/Training/1', f) for f in train_1_fnames]\nsign2_files = [os.path.join('/kaggle/working/Training/2', f) for f in train_2_fnames]\n\nimg_path = random.choice(sign1_files + sign2_files)\nimg = load_img(img_path, target_size=(60, 60))  # this is a PIL image\n\nx   = img_to_array(img)                          \nx   = x.reshape((1,) + x.shape)                  \n\n# Rescale by 1/255\nx /= 255.0\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers]\n\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  \n  if len(feature_map.shape) == 4:\n    \n    n_features = feature_map.shape[-1]  # number of features in the feature map\n    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n    \n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    \n    #-------------------------------------------------\n    # Postprocess the feature to be visually palatable\n    #-------------------------------------------------\n    for i in range(n_features):\n      x  = feature_map[0, :, :, i]\n      x -= x.mean()\n      x /= x.std ()\n      x *=  64\n      x += 128\n      x  = np.clip(x, 0, 255).astype('uint8')\n      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n\n    #-----------------\n    # Display the grid\n    #-----------------\n\n    scale = 20. / n_features\n    plt.figure( figsize=(scale * n_features, scale) )\n    plt.title ( layer_name )\n    plt.grid  ( False )\n    plt.imshow( display_grid, aspect='auto', cmap='viridis' )\n","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:30:13.55588Z","iopub.execute_input":"2021-10-31T18:30:13.556348Z","iopub.status.idle":"2021-10-31T18:30:17.067234Z","shell.execute_reply.started":"2021-10-31T18:30:13.556311Z","shell.execute_reply":"2021-10-31T18:30:17.066342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot the training/validation accuracy ","metadata":{}},{"cell_type":"code","source":"\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'   )","metadata":{"execution":{"iopub.status.busy":"2021-10-31T18:30:17.070219Z","iopub.execute_input":"2021-10-31T18:30:17.072223Z","iopub.status.idle":"2021-10-31T18:30:17.643345Z","shell.execute_reply.started":"2021-10-31T18:30:17.072182Z","shell.execute_reply":"2021-10-31T18:30:17.642603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def grad_cam(input_model, img, layer_name):\n    grad_model = Model([input_model.inputs], [input_model.get_layer(layer_name).output, input_model.output])\n    \n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(np.array([img]))\n        loss = predictions[:, np.argmax(predictions[0])]\n    \n    output = conv_outputs[0]\n    grads = tape.gradient(loss, conv_outputs)[0]\n    \n    # Average pooling of the gradients\n    weights = np.mean(grads, axis=(0, 1))\n    \n    cam = np.dot(output, weights)\n    \n    # Resize the CAM to the size of the image\n    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    cam = np.maximum(cam, 0)\n    cam = cam / cam.max()  # Normalize to [0, 1]\n    \n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    overlay = heatmap + np.float32(img)\n    overlay = overlay / np.max(overlay)\n    \n    return np.uint8(255 * overlay)\n\n\nimg = cv2.imread(img_path)\nimg = cv2.resize(img, (224, 224))  # Resizing to match input shape of the model\n\n# Generate GRAD-CAM heatmap for the last convolutional layer\nheatmap = grad_cam(model, img, layer_name='conv2d_93')  # Example layer name; change based on the model\n\n# Display the heatmap\nplt.imshow(cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB))\nplt.title(\"GRAD-CAM\")\nplt.axis('off')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = (224, 224)  # Image size for the models\nbatch_size = 32\n\n# Data Preprocessing\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntrain_generator = train_datagen.flow_from_directory(\n    dataset_dir, \n    target_size=image_size, \n    batch_size=batch_size, \n    class_mode='categorical', \n    shuffle=False)\n\n# 1. Define Models\n\n# GoogleNet (InceptionV3)\ndef create_googlenet(input_shape=(224, 224, 3), num_classes=43):\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(1024, activation='relu')(x)\n    x = layers.Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# ResNet-34 (using ResNet50 as a close alternative)\ndef create_resnet(input_shape=(224, 224, 3), num_classes=43):\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(1024, activation='relu')(x)\n    x = layers.Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# VGG-16\ndef create_vgg16(input_shape=(224, 224, 3), num_classes=43):\n    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    x = layers.GlobalAveragePooling2D()(base_model.output)\n    x = layers.Dense(1024, activation='relu')(x)\n    x = layers.Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=x)\n    return model\n\n# Simple AlexNet Definition\ndef create_alexnet(input_shape=(224, 224, 3), num_classes=43):\n    model = tf.keras.Sequential([\n        layers.Conv2D(96, kernel_size=11, strides=4, activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n        layers.Conv2D(256, kernel_size=5, padding='same', activation='relu'),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n        layers.Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n        layers.Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n        layers.Conv2D(256, kernel_size=3, padding='same', activation='relu'),\n        layers.MaxPooling2D(pool_size=3, strides=2),\n        layers.Flatten(),\n        layers.Dense(4096, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(4096, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    return model\n\n# Example for DarkNet-53 and MicronNet-BF could be implemented similarly.\n\n# Choose the model to train (you can swap between models as needed)\nmodel = create_resnet()  # You can swap with create_googlenet(), create_vgg16(), create_alexnet(), etc.\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model or load pre-trained model\nmodel.fit(train_generator, epochs=10)\n# model.load_weights('your_pretrained_model.h5')\n\n# 2. GRAD-CAM Function\ndef grad_cam(input_model, img_array, layer_name):\n    grad_model = Model([input_model.inputs], [input_model.get_layer(layer_name).output, input_model.output])\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        loss = predictions[:, np.argmax(predictions[0])]\n    grads = tape.gradient(loss, conv_outputs)[0]\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    \n    for i in range(pooled_grads.shape[-1]):\n        conv_outputs[:, :, i] *= pooled_grads[i]\n    \n    heatmap = tf.reduce_mean(conv_outputs, axis=-1)\n    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n    return heatmap.numpy()\n\n# 3. Apply GRAD-CAM to All Images in the Dataset\ndef apply_grad_cam_to_dataset(model, generator, output_dir, layer_name):\n    for i, (img_batch, label_batch) in enumerate(generator):\n        for j in range(img_batch.shape[0]):\n            img_array = np.expand_dims(img_batch[j], axis=0)\n            heatmap = grad_cam(model, img_array, layer_name=layer_name)\n            \n            # Resize heatmap to image size and overlay\n            heatmap = cv2.resize(heatmap, (image_size[0], image_size[1]))\n            heatmap = np.uint8(255 * heatmap)\n            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n            \n            # Original image\n            img = np.uint8(255 * img_batch[j])\n            superimposed_img = heatmap * 0.4 + img\n\n            # Save image\n            image_filename = f\"grad_cam_{i}_{j}.jpg\"\n            output_path = os.path.join(output_dir, image_filename)\n            cv2.imwrite(output_path, superimposed_img)\n        \n        if i >= len(generator) - 1:  # End the loop when the dataset is done\n            break\n\n# Specify the last convolutional layer of the model you want to visualize\n# For ResNet-34, use 'conv5_block3_out', for GoogleNet use the layer name 'conv2d_93' (change based on architecture)\napply_grad_cam_to_dataset(model, train_generator, output_dir, layer_name='conv5_block3_out')\n\nprint(\"GRAD-CAM visualizations generated and saved.\")\n","metadata":{},"execution_count":null,"outputs":[]}]}